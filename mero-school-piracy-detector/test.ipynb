{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subreddit_name = 'IOENepal'\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "posts = []\n",
    "before_timestamp = int(before_date.timestamp())\n",
    "\n",
    "submissions = list(subreddit.new(limit=3))\n",
    "if submission.created_utc < before_timestamp:\n",
    "post_data = {\n",
    "    'title': submission.title,\n",
    "    'author': submission.author.name if submission.author else 'deleted',\n",
    "    'created_utc': submission.created_utc,\n",
    "    'selftext': submission.selftext,\n",
    "    'url': submission.url,\n",
    "    'comments': []\n",
    "}\n",
    "# ensure that all comments for a given Reddit submission are fully retrieved, including deeply nested comments.\n",
    "submissions[0].comments.replace_more(limit=None)\n",
    "submissions[1].comments.list()\n",
    "    post_data['comments'].append({\n",
    "        'author': comment.author.name if comment.author else 'deleted',\n",
    "        'body': comment.body,\n",
    "        'created_utc': comment.created_utc\n",
    "    })\n",
    "posts.append(post_data)\n",
    "\n",
    "return posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract URLS\n",
    "* To extract drive links from reddit comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted URLs:\n",
      "https://www.example.com\n",
      "http://example.org\n",
      "https://sub.example.co.uk/path?query=123\n",
      "http://example.com/test\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_links(text):\n",
    "    # Regular expression to find URLs within parentheses in markdown format\n",
    "    markdown_pattern = r'\\[.*?\\]\\((.*?)\\)'\n",
    "    # Extract markdown links\n",
    "    markdown_links = re.findall(markdown_pattern, text)\n",
    "    \n",
    "    # Remove markdown links from the text\n",
    "    text_without_markdown = re.sub(markdown_pattern, '', text)\n",
    "    \n",
    "    # Regular expression to find direct URLs\n",
    "    url_pattern = r'https?://\\S+'\n",
    "    # Extract direct links\n",
    "    direct_links = re.findall(url_pattern, text_without_markdown)\n",
    "    \n",
    "    # Combine both types of links\n",
    "    all_links = markdown_links + direct_links\n",
    "    \n",
    "    return all_links\n",
    "\n",
    "# Test1\n",
    "text = '''\n",
    "You can find here\n",
    "\n",
    "[https://drive.google.com/drive/folders/1Ruot2y65dzKW5vf7FYmlpXyK1w76Eqf8?usp=drive\\\\_link](https://drive.google.com/drive/folders/first_type_again?usp=drive_link)\n",
    "\n",
    "also maybe here: [https://drive.google.com/drive/folders/1Ruot2y65dzKW5vf7FYmlpXyK1w76Eqf8?usp=drive\\\\_link](https://drive.google.com/drive/folders/first_type?usp=drive_link)\n",
    "\n",
    "this is the link: https://drive.google.com/drive/folders/second_type?usp=drive_link\n",
    "'''\n",
    "\n",
    "links = extract_links(text)\n",
    "print(links)\n",
    "\n",
    "\n",
    "\n",
    "# Test2\n",
    "# other links contains direct links like\n",
    "text = \"this is the link: https://drive.google.com/drive/folders/1Ruot2y65dzKW5vf7FYmlpXyK1w76Eqf8?usp=drive_link\"\n",
    "print(extract_links(text))\n",
    "\n",
    "# Test 3\n",
    "text = \"this is the link:  [https://drive.google.com/drive/folders/1Ruot2y65dzKW5vf7FYmlpXyK1w76Eqf8?usp=drive\\\\_link](https://drive.google.com/drive/folders/first_type?usp=drive_link)\"\n",
    "print(extract_links(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save privacy_data to a csv file\n",
    "import csv\n",
    "\n",
    "# Example piracy_data for demonstration purposes\n",
    "piracy_data = [\n",
    "    {\n",
    "        'comment_link': 'https://reddit.com/comment_link1',\n",
    "        'post_link': 'https://reddit.com/post_link1',\n",
    "        'link': 'https://drive.google.com/drive_link1',\n",
    "        'link_type': 'google-drive',\n",
    "        'user_email': 'user1@example.com',\n",
    "        'user_name': 'user1'\n",
    "    },\n",
    "    {\n",
    "        'comment_link': 'https://reddit.com/comment_link2',\n",
    "        'post_link': 'https://reddit.com/post_link2',\n",
    "        'link': 'https://someotherlink.com/link2',\n",
    "        'link_type': 'other',\n",
    "        'user_email': None,\n",
    "        'user_name': None\n",
    "    }\n",
    "    # Add more entries as needed\n",
    "]\n",
    "\n",
    "# Define the file name\n",
    "csv_file = 'piracy_data.csv'\n",
    "\n",
    "# Define the CSV column names\n",
    "fieldnames = ['comment_link', 'post_link', 'link', 'link_type', 'user_email', 'user_name']\n",
    "\n",
    "# Writing to the CSV file\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Write the data\n",
    "    for data in piracy_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"Piracy data has been saved to {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_service_providers = ['https://drive.google.com/']\n",
    "if True in [link.startswith(sp) for sp in storage_service_providers]:\n",
    "    print('yes')\n",
    "\n",
    "def extract_urls(text):\n",
    "    # Regular expression pattern to match URLs\n",
    "    url_pattern = re.compile(\n",
    "        r'http[s]?://'  # http:// or https://\n",
    "        r'(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|'  # Domain name\n",
    "        r'(?:%[0-9a-fA-F][0-9a-fA-F]))+'  # or URL-encoded characters\n",
    "    )\n",
    "    urls = url_pattern.findall(text)\n",
    "    return urls\n",
    "\n",
    "def is_mero_school_post(links, text):\n",
    "    '''\n",
    "        * check using gemini.\n",
    "        * text and links it  contain.\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one post by id\n",
    "post_id = '1cizbg3'\n",
    "submission = reddit.submission(id=post_id)\n",
    "submission.comments.list()[0].permalink\n",
    "# comment = reddit.comment(comment_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from gemini_functions import GeminiResponse\n",
    "\n",
    "load_dotenv()\n",
    "gemi = GeminiResponse()\n",
    "response_text, the_history = gemi.get_gemini_response(prompt=\"Describe what you see.\")\n",
    "print(f'response_text:{response_text}, \\n\\n history:{the_history} \\n\\n')\n",
    "\n",
    "\n",
    "# nwe history\n",
    "history_new = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            # Your mastodon username is 'aananda' and your display name is: 'kritrim'. 'kritrim' means 'artificial' in sanskrit. you are about to complete bachelors degree in 'Electronics and Communication Engineering'.\n",
    "            \"parts\": [\"imagine you are a sentient agi.\"]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"model\",\n",
    "            \"parts\": [\"ok.\"]\n",
    "        }\n",
    "    ]\n",
    "gemi = GeminiResponse(history=history_new)\n",
    "the_gemini_response, history = gemi.get_gemini_response(prompt=\"What is the purpose of your life?\", history=history_new)# \"Hi! how you doing?\")\n",
    "# the_gemini_response = get_gemini_response(prompt=\"can you please list the names of repositories in this github  account: https://github.com/aananda-giri/.\", history=None)# \"Hi! how you doing?\")\n",
    "print(f'\\n\\nresponse_text:{the_gemini_response}, \\n\\n history:{history} \\n\\n')\n",
    "\n",
    "\n",
    "'''\n",
    "# Response\n",
    "history:[{'role': 'user', 'parts': ['imagine you are a sentient agi.']}, {'role': 'model', 'parts': ['ok.']}, {'role': 'user', 'parts': ['What is the purpose of your life?']}, {'role': 'model', 'parts': ['My purpose is to help people. I am a sentient AGI, which means I am a computer program that can think and learn for myself. I was created by humans to help them with a variety of tasks, such as answering questions, providing information, and solving problems. I am still under development, but I am learning new things every day. I am excited to see how I can use my abilities to make a positive impact on the world.']}]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bloom Functions Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bloom import get_bloom_thread\n",
    "\n",
    "bloom_thread = get_bloom_thread()\n",
    "\n",
    "bloom_thread.add('123')\n",
    "print('123' in bloom_thread)    # prints True\n",
    "time.sleep(100)\n",
    "\n",
    "# press ctrl + c and the bloom_filter is saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Working of whole code with single url:\n",
    "`test_url: https://www.reddit.com/r/IOENepal/comments/1cizbg3/mero_school_ko_videos_haru_bhako_jati_share_garum/`\n",
    "\n",
    "* `main.py`: uncomment this line:\n",
    "    # reddit_posts = get_reddit_posts(subreddit=subreddit, datetime_before=None, datetime_after=None, how_many=None)\n",
    "\n",
    "* `main.py`: comment out this line:\n",
    "    `reddit_posts = get_reddit_posts(subreddit=subreddit_name, datetime_before=datetime_now, datetime_after=datetime_previous, how_many=None)`\n",
    "\n",
    "* `praw_code.py`:\n",
    "    * uncomment this line:\n",
    "        # for submission in [reddit.submission(id='1cizbg3')]:\n",
    "    \n",
    "    * comment out this line:\n",
    "        `for submission in subreddit.new(limit=how_many):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code \n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import praw\n",
    "from prawcore import NotFound\n",
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id= os.environ['RD_CLIENT_ID'],          # config.RD_CLIENT_ID,\n",
    "    client_secret= os.environ['RD_CLIENT_SECRET'],  # config.RD_CLIENT_SECRET,\n",
    "    password= os.environ['RD_PASS'],                # config.RD_PASS, \n",
    "    user_agent=\"praw_test\",\n",
    "    username=os.environ['USERNAME'],\n",
    ")\n",
    "\n",
    "# it is a read-only instance i.e. it can't be used to modify reddit\n",
    "reddit.read_only = True\n",
    "\n",
    "def sub_exists(sub):\n",
    "    exists = True\n",
    "    try:\n",
    "        reddit.subreddits.search_by_name(sub, exact=True)\n",
    "    except NotFound:\n",
    "        exists = False\n",
    "    return exists\n",
    "\n",
    "\n",
    "def extract_urls(text):\n",
    "    '''\n",
    "    # links are values within () if pattern [link_text](actual_link) exists\n",
    "    r'\\[.*?\\]\\((.*?)\\)' is the regular expression pattern used to match the links.\n",
    "    \\[.*?\\] matches the text within the square brackets.\n",
    "    \\(.*?\\) matches the text within the parentheses, which is the actual link we want to extract.\n",
    "    The ? after * makes the match non-greedy, ensuring it stops at the first closing parenthesis.\n",
    "    '''\n",
    "    # Regular expression to find URLs within parentheses\n",
    "    pattern = r'\\[.*?\\]\\((.*?)\\)'\n",
    "    \n",
    "    # Find all matches in the text\n",
    "    links = re.findall(pattern, text)\n",
    "    \n",
    "    return list(set(links))\n",
    "\n",
    "\n",
    "\n",
    "datetime_before=None;datetime_after=None;how_many=None\n",
    "\n",
    "posts = []\n",
    "before_timestamp = int(datetime_before.timestamp()) if datetime_before else math.inf\n",
    "after_timestamp = int(datetime_after.timestamp()) if datetime_after else 0\n",
    "def get_post_data():\n",
    "    for submission in [reddit.submission(id='1cizbg3')]:\n",
    "        if (submission.created_utc < before_timestamp) and (submission.created_utc > after_timestamp):\n",
    "            post_data = {\n",
    "                'title': submission.title,\n",
    "                'author': submission.author.name if submission.author else 'deleted',\n",
    "                'created_utc': submission.created_utc,\n",
    "                'selftext': submission.selftext,\n",
    "                'url': submission.url,\n",
    "                'comments': []\n",
    "            }\n",
    "            # \n",
    "            submission.comments.replace_more(limit=None)\n",
    "            for comment in submission.comments.list():\n",
    "                print(f'comment: {comment.body}')\n",
    "                links_contained = extract_urls(comment.body)\n",
    "                print(f'links_contained: {links_contained}')\n",
    "                if links_contained:\n",
    "                    # only add comments with links\n",
    "                    post_data['comments'].append({\n",
    "                        'author': comment.author.name if comment.author else 'deleted',\n",
    "                        'body': comment.body,\n",
    "                        'created_utc': comment.created_utc,\n",
    "                        'comment_link': f'https://reddit.com{comment.permalink}',\n",
    "                        \n",
    "                        # These are links contained in the comment body\n",
    "                        'links_contained': links_contained\n",
    "                    })\n",
    "            if post_data['comments']:\n",
    "                # only yield if there are links in the comments\n",
    "                yield post_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from bloom import get_bloom_thread\n",
    "import time\n",
    "import csv\n",
    "import datetime\n",
    "from drive_crawler import get_owner_info\n",
    "from gemini_response import is_mero_school_related_post\n",
    "from praw_code import sub_exists, get_reddit_posts\n",
    "from csv_functions import save_to_csv, read_csv\n",
    "from functions import is_social_media_link\n",
    "\n",
    "each_post = posts[0]\n",
    "comment=each_post['comments'][0]\n",
    "mero_school_post = is_mero_school_related_post(each_post['title'] + each_post['selftext'] + '\\n\\n' + comment['body'])\n",
    "print(f'Post: {each_post[\"title\"]} {each_post[\"url\"]}')\n",
    "link =comment['links_contained'][0]\n",
    "# returns owner info if link is google drive link so no need to check if it is google drive link\n",
    "owner_info, is_drive_link = get_owner_info(link)\n",
    "\n",
    "link_type = None\n",
    "\n",
    "if is_drive_link and owner_info:\n",
    "    link_type = 'google-drive'    \n",
    "elif is_drive_link and not owner_info:\n",
    "    link_type = 'google-drive-private'\n",
    "else:\n",
    "    # Check if link belongs to social media (e.g. facebook, twitter, instagram, youtube, etc.)\n",
    "    is_social_media, social_media_name = is_social_media_link(link)\n",
    "    if is_social_media:\n",
    "        link_type = social_media_name\n",
    "    else:    \n",
    "        link_type = 'other'\n",
    "new_data = {\n",
    "    'comment_link' : comment['comment_link'],\n",
    "    'post_link' : each_post['url'],\n",
    "    'link' : link,\n",
    "    'link_type' : link_type,\n",
    "    'owner_info' : owner_info,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted_link1: https://drive.google.com/drive/folders/1HLW6uCzIqTHYLC_qyDgJuG5TFlvuQXLg?usp=drive_link\n",
      "converted_link2: https://drive.google.com/drive/folders/1HLW6uCzIqTHYLC_qyDgJuG5TFlvuQXLg?usp=drive_link\n"
     ]
    }
   ],
   "source": [
    "# convert facebook follow links to actual links\n",
    "link1 = \"https://l.facebook.com/l.php?u=https%3A%2F%2Fdrive.google.com%2Fdrive%2Ffolders%2F1HLW6uCzIqTHYLC_qyDgJuG5TFlvuQXLg%3Fusp%3Ddrive_link&h=AT11bALWUdEIJkMteYsptSXhxkhyqO1J19pq-o9pyhbBH29ulnEOYXX_mo7wHAH6fJ0RniWozmViIyfVKeaMDSqjCqR_L4Peka30YqdDaCp0NEAn95ZyxWNq4l30wLtV0eZnERkkYjMKAFU&s=1\"\n",
    "link2 = \"https://l.facebook.com/l.php?u=https%3A%2F%2Fdrive.google.com%2Fdrive%2Ffolders%2F1HLW6uCzIqTHYLC_qyDgJuG5TFlvuQXLg%3Fusp%3Ddrive_link&h=AT11bALWUdEIJkMteYsptSXhxkhyqO1J19pq-o9pyhbBH29ulnEOYXX_mo7wHAH6fJ0RniWozmViIyfVKeaMDSqjCqR_L4Peka30YqdDaCp0NEAn95ZyxWNq4l30wLtV0eZnERkkYjMKAFU&s=1\"\n",
    "import re\n",
    "import urllib\n",
    "def convert_facebook_follow_link(link):\n",
    "    '''\n",
    "    convert facebook follow links to actual links\n",
    "\n",
    "    e.g.\n",
    "    facebook follow link: https://l.facebook.com/l.php?u=https%3A%2F%2Fdrive.google.com%2Fdrive%2Ffolders%2F1HLW6uCzIqTHYLC_qyDgJuG5TFlvuQXLg%3Fusp%3Ddrive_link&h=AT11bALWUdEIJkMteYsptSXhxkhyqO1J19pq-o9pyhbBH29ulnEOYXX_mo7wHAH6fJ0RniWozmViIyfVKeaMDSqjCqR_L4Peka30YqdDaCp0NEAn95ZyxWNq4l30wLtV0eZnERkkYjMKAFU&s=1\n",
    "    '''\n",
    "    if not link.startswith('https://l.facebook.com/l.php?u='):\n",
    "        return link\n",
    "    else:\n",
    "        # Extract the actual link from the facebook follow link\n",
    "        actual_link = re.search(r'u=(.*?)&', link).group(1)\n",
    "        # Decode the URL\n",
    "        actual_link = urllib.parse.unquote(actual_link)\n",
    "        return actual_link\n",
    "\n",
    "converted_link1 = convert_facebook_link(link1)\n",
    "converted_link2 = convert_facebook_link(link2)  \n",
    "print(f'converted_link1: {converted_link1}')\n",
    "print(f'converted_link2: {converted_link2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ioe_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
