{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subreddit_name = 'IOENepal'\n",
    "subreddit = reddit.subreddit(subreddit_name)\n",
    "posts = []\n",
    "before_timestamp = int(before_date.timestamp())\n",
    "\n",
    "submissions = list(subreddit.new(limit=3))\n",
    "if submission.created_utc < before_timestamp:\n",
    "post_data = {\n",
    "    'title': submission.title,\n",
    "    'author': submission.author.name if submission.author else 'deleted',\n",
    "    'created_utc': submission.created_utc,\n",
    "    'selftext': submission.selftext,\n",
    "    'url': submission.url,\n",
    "    'comments': []\n",
    "}\n",
    "# ensure that all comments for a given Reddit submission are fully retrieved, including deeply nested comments.\n",
    "submissions[0].comments.replace_more(limit=None)\n",
    "submissions[1].comments.list()\n",
    "    post_data['comments'].append({\n",
    "        'author': comment.author.name if comment.author else 'deleted',\n",
    "        'body': comment.body,\n",
    "        'created_utc': comment.created_utc\n",
    "    })\n",
    "posts.append(post_data)\n",
    "\n",
    "return posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract URLS\n",
    "* To extract drive links from reddit comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted URLs:\n",
      "https://www.example.com\n",
      "http://example.org\n",
      "https://sub.example.co.uk/path?query=123\n",
      "http://example.com/test\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def extract_urls(text):\n",
    "    # Regular expression pattern to match URLs\n",
    "    url_pattern = re.compile(\n",
    "        r'http[s]?://'  # http:// or https://\n",
    "        r'(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|'  # Domain name\n",
    "        r'(?:%[0-9a-fA-F][0-9a-fA-F]))+'  # or URL-encoded characters\n",
    "    )\n",
    "    urls = url_pattern.findall(text)\n",
    "    return urls\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"\n",
    "Here are some URLs:\n",
    "- https://www.example.com\n",
    "- http://example.org\n",
    "- Check this out: https://sub.example.co.uk/path?query=123\n",
    "- Another link: http://example.com/test#anchor\n",
    "\"\"\"\n",
    "\n",
    "urls = extract_urls(text)\n",
    "print(\"Extracted URLs:\")\n",
    "for url in urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save privacy_data to a csv file\n",
    "import csv\n",
    "\n",
    "# Example piracy_data for demonstration purposes\n",
    "piracy_data = [\n",
    "    {\n",
    "        'comment_link': 'https://reddit.com/comment_link1',\n",
    "        'post_link': 'https://reddit.com/post_link1',\n",
    "        'link': 'https://drive.google.com/drive_link1',\n",
    "        'link_type': 'google-drive',\n",
    "        'user_email': 'user1@example.com',\n",
    "        'user_name': 'user1'\n",
    "    },\n",
    "    {\n",
    "        'comment_link': 'https://reddit.com/comment_link2',\n",
    "        'post_link': 'https://reddit.com/post_link2',\n",
    "        'link': 'https://someotherlink.com/link2',\n",
    "        'link_type': 'other',\n",
    "        'user_email': None,\n",
    "        'user_name': None\n",
    "    }\n",
    "    # Add more entries as needed\n",
    "]\n",
    "\n",
    "# Define the file name\n",
    "csv_file = 'piracy_data.csv'\n",
    "\n",
    "# Define the CSV column names\n",
    "fieldnames = ['comment_link', 'post_link', 'link', 'link_type', 'user_email', 'user_name']\n",
    "\n",
    "# Writing to the CSV file\n",
    "with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Write the data\n",
    "    for data in piracy_data:\n",
    "        writer.writerow(data)\n",
    "\n",
    "print(f\"Piracy data has been saved to {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_service_providers = ['https://drive.google.com/']\n",
    "if True in [link.startswith(sp) for sp in storage_service_providers]:\n",
    "    print('yes')\n",
    "\n",
    "def extract_urls(text):\n",
    "    # Regular expression pattern to match URLs\n",
    "    url_pattern = re.compile(\n",
    "        r'http[s]?://'  # http:// or https://\n",
    "        r'(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|'  # Domain name\n",
    "        r'(?:%[0-9a-fA-F][0-9a-fA-F]))+'  # or URL-encoded characters\n",
    "    )\n",
    "    urls = url_pattern.findall(text)\n",
    "    return urls\n",
    "\n",
    "def is_mero_school_post(links, text):\n",
    "    '''\n",
    "        * check using gemini.\n",
    "        * text and links it  contain.\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one post by id\n",
    "post_id = '1cizbg3'\n",
    "submission = reddit.submission(id=post_id)\n",
    "submission.comments.list()[0].permalink\n",
    "# comment = reddit.comment(comment_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ioe_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
