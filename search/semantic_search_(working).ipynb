{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1oZhF5LHf_lrBCw-vGWCorsV3fN4uCFxa",
      "authorship_tag": "ABX9TyP0XQPZU1X773lf3JH3nK7n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aananda-giri/scripts/blob/main/semantic_search_(working).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank-bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fmfApZ7ybaB",
        "outputId": "e6d6dad4-0369-4d7b-d6e7-9df84187d799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank-bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank-bm25) (1.23.5)\n",
            "Installing collected packages: rank-bm25\n",
            "Successfully installed rank-bm25-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import click\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "from rank_bm25 import BM25Okapi as BM25\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n"
      ],
      "metadata": {
        "id": "PjhKnjOlykD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qkyjqer3xPzL"
      },
      "outputs": [],
      "source": [
        "class Retriever(object):\n",
        "    def __init__(self, documents):\n",
        "        self.corpus = documents\n",
        "        self.bm25 = BM25(self.corpus)\n",
        "\n",
        "    def query(self, tokenized_query, n=100):\n",
        "        scores = self.bm25.get_scores(tokenized_query)\n",
        "        best_docs = sorted(range(len(scores)), key=lambda i: -scores[i])[:n]\n",
        "        return best_docs, [scores[i] for i in best_docs]\n",
        "\n",
        "\n",
        "class Ranker(object):\n",
        "    def __init__(self, query_embedding, document_embedding):\n",
        "        self.query_embedding = query_embedding\n",
        "        self.document_embedding = document_embedding\n",
        "\n",
        "    def _create_mean_embedding(self, word_embeddings):\n",
        "        return np.mean(\n",
        "            word_embeddings,\n",
        "            axis=0,\n",
        "        )\n",
        "\n",
        "    def _create_max_embedding(self, word_embeddings):\n",
        "        return np.amax(\n",
        "            word_embeddings,\n",
        "            axis=0,\n",
        "        )\n",
        "\n",
        "    def _embed(self, tokens, embedding):\n",
        "        word_embeddings = np.array([embedding[token] for token in tokens if token in embedding])\n",
        "        mean_embedding = self._create_mean_embedding(word_embeddings)\n",
        "        max_embedding = self._create_max_embedding(word_embeddings)\n",
        "        embedding = np.concatenate([mean_embedding, max_embedding])\n",
        "        unit_embedding = embedding / (embedding**2).sum()**0.5\n",
        "        return unit_embedding\n",
        "\n",
        "    def rank(self, tokenized_query, tokenized_documents):\n",
        "        \"\"\"\n",
        "        Re-ranks a set of documents according to embedding distance\n",
        "        \"\"\"\n",
        "        query_embedding = self._embed(tokenized_query, self.query_embedding) # (E,)\n",
        "        document_embeddings = np.array([self._embed(document, self.document_embedding) for document in tokenized_documents]) # (N, E)\n",
        "        scores = document_embeddings.dot(query_embedding)\n",
        "        index_rankings = np.argsort(scores)[::-1]\n",
        "        return index_rankings, np.sort(scores)[::-1]\n",
        "\n",
        "\n",
        "class TSVDocumentReader(object):\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "\n",
        "    @property\n",
        "    def corpus(self):\n",
        "        df = pd.read_csv(self.path, delimiter=\"\\t\", header=None)\n",
        "        return df[3].values.tolist()\n",
        "\n",
        "class DocumentReader(object):\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "\n",
        "    @property\n",
        "    def corpus(self):\n",
        "        documents = []\n",
        "        glob_path = os.path.join(self.path, \"**\")\n",
        "        for document_path in glob.glob(glob_path, recursive=True):\n",
        "            if os.path.isfile(document_path):\n",
        "                with open(document_path, 'r', encoding='ISO-8859-1') as f:\n",
        "                    documents.append(f.read())\n",
        "        return documents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a TSV File\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data for the TSV file\n",
        "data = {\n",
        "    \"Title\": [\"Article 1\", \"Article 2\", \"Article 3\", \"Article 4\", \"Article 5\"],\n",
        "    \"Description\": [\"An investment bonanza is coming\", \"Who governs a country’s airspace?\",\n",
        "                    \"What is a supermoon, and how noticeable is it to the naked eye?\", \"What the evidence says about police body-cameras?\",\n",
        "                    \"Who controls Syria?\"],\n",
        "    \"Category\": [\"Science\", \"Technology\", \"Health\", \"Education\", \"Environment\"],\n",
        "    \"Content\": [\n",
        "        \"An investment bonanza is coming. This is the content of the first article, discussing recent scientific discoveries. \",\n",
        "        \"Who governs a country’s airspace? The second article covers the latest advancements in technology and its applications.\",\n",
        "        \"What is a supermoon, and how noticeable is it to the naked eye? Healthcare improvements and medical research are the main topics of the third article.\",\n",
        "        \"What the evidence says about police body-cameras? This article focuses on educational reforms and learning methodologies.\",\n",
        "        \"Who controls Syria? Environmental issues and sustainability practices are explored in this article.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save as a TSV file\n",
        "tsv_path = '/content/sample_documents.tsv'\n",
        "df.to_csv(tsv_path, sep='\\t', index=False, header=False)\n",
        "\n",
        "tsv_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1a7CNOvj63P6",
        "outputId": "a5abc3a3-8fc6-4f42-8f31-a16088d56052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/sample_documents.tsv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tokenize(document):\n",
        "    return list(gensim.utils.tokenize(document.lower()))\n",
        "\n",
        "\n",
        "def show_scores(documents, scores, n=10):\n",
        "    for i in range(n):\n",
        "        print(\"======== RANK: {} | SCORE: {} =======\".format(i + 1, scores[i]))\n",
        "        print(documents[i])\n",
        "        print(\"\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# @click.command()\n",
        "# @click.option(\"--path\", prompt=\"Path to document TSV\", help=\"Document TSV\")\n",
        "# @click.option(\"--query\", prompt=\"Search query\", help=\"Search query\")\n",
        "def main(path=\"/content/sample_documents.tsv\", query=\"astrology\"):\n",
        "    print('Query: \"{}\"'.format(query))\n",
        "\n",
        "    print(\"Reading documents...\", end=\"\")\n",
        "    reader = TSVDocumentReader(path)\n",
        "    documents = [doc for doc in reader.corpus]\n",
        "    print(\" [DONE]\")\n",
        "    print(\"Tokening documents...\", end=\"\")\n",
        "    corpus = [list(gensim.utils.tokenize(doc.lower())) for doc in documents]\n",
        "    tokenized_query = tokenize(query)\n",
        "    print(\" [DONE]\")\n",
        "\n",
        "    retriever = Retriever(corpus)\n",
        "    retrieval_indexes, retrieval_scores = retriever.query(tokenized_query)\n",
        "\n",
        "    retrieved_documents = [documents[idx] for idx in retrieval_indexes]\n",
        "    print(\"======== BM25 ========\")\n",
        "    show_scores(retrieved_documents, retrieval_scores, 5)\n",
        "\n",
        "    tokenzed_retrieved_documents = [corpus[idx] for idx in retrieval_indexes]\n",
        "\n",
        "    print(\"Loading glove embeddings...\", end=\"\")\n",
        "    query_embedding = api.load('glove-wiki-gigaword-50')\n",
        "    print(\" [DONE]\")\n",
        "    ranker = Ranker(query_embedding=query_embedding, document_embedding=query_embedding)\n",
        "    ranker_indexes, ranker_scores = ranker.rank(tokenized_query, tokenzed_retrieved_documents)\n",
        "    reranked_documents = [retrieved_documents[idx] for idx in ranker_indexes]\n",
        "\n",
        "    print(\"======== Embedding ========\")\n",
        "    show_scores(reranked_documents, ranker_scores, 5)\n",
        "\n",
        "    print(\"======== Samples ========\")\n",
        "    documents = [\n",
        "        \"An investment bonanza is coming\",\n",
        "        \"Who governs a country's airspace?\",\n",
        "        \"What is a supermoon, and how noticeable is it to the naked eye?\",\n",
        "        \"What the evidence says about police body-cameras\",\n",
        "        \"Who controls Syria?\",\n",
        "    ]\n",
        "    corpus = [list(gensim.utils.tokenize(doc.lower())) for doc in documents]\n",
        "    queries = [\n",
        "        \"banking\",\n",
        "        \"astrology\",\n",
        "        \"middle east\",\n",
        "    ]\n",
        "    for query in queries:\n",
        "        tokenized_query = tokenize(query)\n",
        "        indexes, scores = ranker.rank(tokenized_query, corpus)\n",
        "        print(query)\n",
        "        for rank, index in enumerate(indexes):\n",
        "            document = documents[index]\n",
        "            print(\"Rank: {} | Top Article: {}\".format(rank, document))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gsx7jhzIyvYV",
        "outputId": "8adba9a2-14b7-4ddf-8b82-c2e196e5193f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: \"astrology\"\n",
            "Reading documents... [DONE]\n",
            "Tokening documents... [DONE]\n",
            "======== BM25 ========\n",
            "======== RANK: 1 | SCORE: 0.0 =======\n",
            "An investment bonanza is coming. This is the content of the first article, discussing recent scientific discoveries. \n",
            "\n",
            "======== RANK: 2 | SCORE: 0.0 =======\n",
            "Who governs a country’s airspace? The second article covers the latest advancements in technology and its applications.\n",
            "\n",
            "======== RANK: 3 | SCORE: 0.0 =======\n",
            "What is a supermoon, and how noticeable is it to the naked eye? Healthcare improvements and medical research are the main topics of the third article.\n",
            "\n",
            "======== RANK: 4 | SCORE: 0.0 =======\n",
            "What the evidence says about police body-cameras? This article focuses on educational reforms and learning methodologies.\n",
            "\n",
            "======== RANK: 5 | SCORE: 0.0 =======\n",
            "Who controls Syria? Environmental issues and sustainability practices are explored in this article.\n",
            "\n",
            "\n",
            "\n",
            "Loading glove embeddings... [DONE]\n",
            "======== Embedding ========\n",
            "======== RANK: 1 | SCORE: 0.23088905215263367 =======\n",
            "Who controls Syria? Environmental issues and sustainability practices are explored in this article.\n",
            "\n",
            "======== RANK: 2 | SCORE: 0.21348285675048828 =======\n",
            "An investment bonanza is coming. This is the content of the first article, discussing recent scientific discoveries. \n",
            "\n",
            "======== RANK: 3 | SCORE: 0.19887691736221313 =======\n",
            "What is a supermoon, and how noticeable is it to the naked eye? Healthcare improvements and medical research are the main topics of the third article.\n",
            "\n",
            "======== RANK: 4 | SCORE: 0.19782692193984985 =======\n",
            "What the evidence says about police body-cameras? This article focuses on educational reforms and learning methodologies.\n",
            "\n",
            "======== RANK: 5 | SCORE: 0.19091355800628662 =======\n",
            "Who governs a country’s airspace? The second article covers the latest advancements in technology and its applications.\n",
            "\n",
            "\n",
            "\n",
            "======== Samples ========\n",
            "banking\n",
            "Rank: 0 | Top Article: An investment bonanza is coming\n",
            "Rank: 1 | Top Article: Who governs a country's airspace?\n",
            "Rank: 2 | Top Article: What is a supermoon, and how noticeable is it to the naked eye?\n",
            "Rank: 3 | Top Article: Who controls Syria?\n",
            "Rank: 4 | Top Article: What the evidence says about police body-cameras\n",
            "astrology\n",
            "Rank: 0 | Top Article: What is a supermoon, and how noticeable is it to the naked eye?\n",
            "Rank: 1 | Top Article: Who governs a country's airspace?\n",
            "Rank: 2 | Top Article: Who controls Syria?\n",
            "Rank: 3 | Top Article: What the evidence says about police body-cameras\n",
            "Rank: 4 | Top Article: An investment bonanza is coming\n",
            "middle east\n",
            "Rank: 0 | Top Article: Who controls Syria?\n",
            "Rank: 1 | Top Article: Who governs a country's airspace?\n",
            "Rank: 2 | Top Article: An investment bonanza is coming\n",
            "Rank: 3 | Top Article: What is a supermoon, and how noticeable is it to the naked eye?\n",
            "Rank: 4 | Top Article: What the evidence says about police body-cameras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download NLTK resources\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Function to preprocess and tokenize text\n",
        "def preprocess(text):\n",
        "    lower = text.lower()\n",
        "    tokens = word_tokenize(lower)\n",
        "    # Remove stopwords and punctuation\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [t for t in tokens if t not in stop_words and t not in string.punctuation]\n",
        "    return tokens\n",
        "\n",
        "# Function to create an averaged word vector for a document\n",
        "def document_vector(doc, model):\n",
        "    doc = [word for word in doc if word in model.key_to_index]\n",
        "    return np.mean(model[doc], axis=0)\n",
        "\n",
        "# Load word embeddings\n",
        "model = api.load('glove-wiki-gigaword-50')\n",
        "\n",
        "# Function to load documents from a TSV file\n",
        "def load_documents(path):\n",
        "    df = pd.read_csv(path, delimiter=\"\\t\", header=None)\n",
        "    return df[3].tolist()  # Assuming the content is in the fourth column\n",
        "\n",
        "# Function to compute cosine similarity\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "# Semantic search function\n",
        "def semantic_search(query, documents, model):\n",
        "    processed_query = preprocess(query)\n",
        "    query_vector = document_vector(processed_query, model)\n",
        "\n",
        "    scores = []\n",
        "    for doc in documents:\n",
        "        processed_doc = preprocess(doc)\n",
        "        doc_vector = document_vector(processed_doc, model)\n",
        "        scores.append(cosine_similarity(query_vector, doc_vector))\n",
        "\n",
        "    sorted_docs = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)\n",
        "    return sorted_docs\n",
        "\n",
        "# Example usage\n",
        "path_to_tsv = '/content/sample_documents.tsv'  # Replace with your TSV file path\n",
        "documents = load_documents(path_to_tsv)\n",
        "\n",
        "# query = \"Enter your search query here\"\n",
        "query = \"Banking\"\n",
        "top_docs = semantic_search(query, documents, model)\n",
        "\n",
        "# Display top 5 documents\n",
        "for doc, score in top_docs[:5]:\n",
        "    print(f\"Score: {score}\\nDocument: {doc}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OE4KMFvBF_Y",
        "outputId": "0d608a2b-1dec-4e8b-8f5e-bf500da1a9db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
            "Score: 0.6276920437812805\n",
            "Document: Healthcare improvements and medical research are the main topics of the third article.\n",
            "\n",
            "Score: 0.5873264074325562\n",
            "Document: Environmental issues and sustainability practices are explored in this article.\n",
            "\n",
            "Score: 0.5339373350143433\n",
            "Document: The second article covers the latest advancements in technology and its applications.\n",
            "\n",
            "Score: 0.4885958731174469\n",
            "Document: This article focuses on educational reforms and learning methodologies.\n",
            "\n",
            "Score: 0.4656783938407898\n",
            "Document: This is the content of the first article, discussing recent scientific discoveries. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/fse/word2vec-google-news-300/resolve/main/word2vec-google-news-300.model?download=true -O /content/drive/MyDrive/Research/word2vec/word2vec-google-news-300.model\n",
        "\n",
        "!wget https://huggingface.co/fse/word2vec-google-news-300/resolve/main/word2vec-google-news-300.model.vectors.npy?download=true -O -O /content/drive/MyDrive/Research/word2vec/word2vec-google-news-300.model.vectors.npy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47_9K7Qe7GB2",
        "outputId": "ceba70e3-c594-4919-8581-56436ba0a927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-25 10:00:59--  https://huggingface.co/fse/word2vec-google-news-300/resolve/main/word2vec-google-news-300.model.vectors.npy?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 13.33.33.102, 13.33.33.20, 13.33.33.110, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.33.33.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/fse/word2vec-google-news-300/f22370268ca0b4fb12567df754f079b4708a189d5f063de19fa19535e91d41de?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27word2vec-google-news-300.model.vectors.npy%3B+filename%3D%22word2vec-google-news-300.model.vectors.npy%22%3B&Expires=1701165659&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTE2NTY1OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9mc2Uvd29yZDJ2ZWMtZ29vZ2xlLW5ld3MtMzAwL2YyMjM3MDI2OGNhMGI0ZmIxMjU2N2RmNzU0ZjA3OWI0NzA4YTE4OWQ1ZjA2M2RlMTlmYTE5NTM1ZTkxZDQxZGU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=m9cQK6JYShPKDcy%7EbUZtbCNXpC0Om0yAbkcxlM5tW8PqZQIm9vV8cLuVqANLV91lKkrPYVuY2i9QRtTqiTkFIsFEOgj1EYM0VLqqOBr6q003yTjqXAmjqk2a0%7EtH6D14DAIhLB55kKcjR5ygGVZbqj74AusAfwMLbm2f5FmkqYobEEkkYXRw7VATkE4noeKm0FTWYM9Ap1EKe24SS0lH0YoV9BH1W-GjfiN3V-xH6lvj7qSNu5mSwk2E5Hyqw%7EJeOzeHKji1T6EXa0LiQB7lhK7G9w4wLPEVEvemZUbXfvCLgVbH9jKTMV4KCUKt24XGpcI8Fwg6tSmRxzoPEqMpPA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-11-25 10:00:59--  https://cdn-lfs.huggingface.co/fse/word2vec-google-news-300/f22370268ca0b4fb12567df754f079b4708a189d5f063de19fa19535e91d41de?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27word2vec-google-news-300.model.vectors.npy%3B+filename%3D%22word2vec-google-news-300.model.vectors.npy%22%3B&Expires=1701165659&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMTE2NTY1OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9mc2Uvd29yZDJ2ZWMtZ29vZ2xlLW5ld3MtMzAwL2YyMjM3MDI2OGNhMGI0ZmIxMjU2N2RmNzU0ZjA3OWI0NzA4YTE4OWQ1ZjA2M2RlMTlmYTE5NTM1ZTkxZDQxZGU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=m9cQK6JYShPKDcy%7EbUZtbCNXpC0Om0yAbkcxlM5tW8PqZQIm9vV8cLuVqANLV91lKkrPYVuY2i9QRtTqiTkFIsFEOgj1EYM0VLqqOBr6q003yTjqXAmjqk2a0%7EtH6D14DAIhLB55kKcjR5ygGVZbqj74AusAfwMLbm2f5FmkqYobEEkkYXRw7VATkE4noeKm0FTWYM9Ap1EKe24SS0lH0YoV9BH1W-GjfiN3V-xH6lvj7qSNu5mSwk2E5Hyqw%7EJeOzeHKji1T6EXa0LiQB7lhK7G9w4wLPEVEvemZUbXfvCLgVbH9jKTMV4KCUKt24XGpcI8Fwg6tSmRxzoPEqMpPA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.33.33.69, 13.33.33.45, 13.33.33.119, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.33.33.69|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3600000128 (3.4G) [application/octet-stream]\n",
            "Saving to: ‘-O’\n",
            "\n",
            "-O                  100%[===================>]   3.35G  14.7MB/s    in 4m 7s   \n",
            "\n",
            "2023-11-25 10:05:07 (13.9 MB/s) - ‘-O’ saved [3600000128/3600000128]\n",
            "\n",
            "/content/drive/MyDrive/Research/word2vec/word2vec-google-news-300.model.vectors.npy: Scheme missing.\n",
            "FINISHED --2023-11-25 10:05:07--\n",
            "Total wall clock time: 4m 8s\n",
            "Downloaded: 1 files, 3.4G in 4m 7s (13.9 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References:\n",
        "\n",
        "* https://dev.to/mage_ai/how-to-build-a-search-engine-with-word-embeddings-56jd\n",
        "\n"
      ],
      "metadata": {
        "id": "P5LYgopHHQKR"
      }
    }
  ]
}