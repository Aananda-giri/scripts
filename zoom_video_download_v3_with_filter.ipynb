{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aananda-giri/scripts/blob/main/zoom_video_download_v3_with_filter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Yf4OfPBAAPR",
        "outputId": "9088360d-cef9-43ef-91c4-22648af12a72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2,240 kB]\n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,049 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,699 kB]\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:14 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,378 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,175 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,344 kB]\n",
            "Fetched 13.2 MB in 2s (5,920 kB/s)\n",
            "Reading package lists... Done\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[34;40mnotice\u001b[0m\u001b[35m\u001b[0m created a lockfile as package-lock.json. You should commit this file.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ convert-excel-to-json@1.7.0\n",
            "added 66 packages from 28 contributors and audited 66 packages in 4.649s\n",
            "found \u001b[91m5\u001b[0m vulnerabilities (4 \u001b[93mmoderate\u001b[0m, 1 \u001b[91mhigh\u001b[0m)\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h\n",
            "> puppeteer@20.2.1 postinstall /content/node_modules/puppeteer\n",
            "> node install.js\n",
            "\n",
            "\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 0% 0.0s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 0% 8.2s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 0% 6.9s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 0% 5.7s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 1% 5.2s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 1% 5.0s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 2% 5.2s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 2% 5.5s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 2% 5.8s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 2% 6.5s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 2% 6.9s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 2% 7.5s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 3% 7.4s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 3% 7.2s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 3% 7.1s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 4% 6.7s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 4% 6.3s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 5% 5.9s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 5% 5.7s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 6% 5.3s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 6% 5.0s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 7% 4.9s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 7% 4.7s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 8% 4.5s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 9% 4.3s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 9% 4.2s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 10% 4.2s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 11% 4.1s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 12% 4.0s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 12% 3.9s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 13% 3.9s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 13% 3.8s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 14% 3.7s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 15% 3.6s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 16% 3.5s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 16% 3.4s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 17% 3.3s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 18% 3.2s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 19% 3.1s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 20% 3.1s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 20% 3.0s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 21% 3.0s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 22% 2.9s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 22% 2.8s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 23% 2.8s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 24% 2.7s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 25% 2.6s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 26% 2.6s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 26% 2.5s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 27% 2.5s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 28% 2.5s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 28% 2.4s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 29% 2.4s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 30% 2.3s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 31% 2.3s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 31% 2.2s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 32% 2.2s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 33% 2.2s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 33% 2.1s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 34% 2.1s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 35% 2.1s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 36% 2.0s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 37% 2.0s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 38% 2.0s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 38% 1.9s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 39% 1.9s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 40% 1.9s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 41% 1.8s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 42% 1.8s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 43% 1.8s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 43% 1.7s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 44% 1.7s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 45% 1.7s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 46% 1.6s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 47% 1.6s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 48% 1.5s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 49% 1.5s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 50% 1.4s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 51% 1.4s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 52% 1.4s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 53% 1.3s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 54% 1.3s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 55% 1.3s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 56% 1.2s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 57% 1.2s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 58% 1.2s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 59% 1.1s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 60% 1.1s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 61% 1.1s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 62% 1.0s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 63% 1.0s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 64% 1.0s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 65% 0.9s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 66% 0.9s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 67% 0.9s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 68% 0.8s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 69% 0.8s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 70% 0.8s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 71% 0.8s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 71% 0.7s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 72% 0.7s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 73% 0.7s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 74% 0.7s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 75% 0.6s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 76% 0.6s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 77% 0.6s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 78% 0.6s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 79% 0.5s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 80% 0.5s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 81% 0.5s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 82% 0.4s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 83% 0.4s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 84% 0.4s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 85% 0.4s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 86% 0.3s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 87% 0.3s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 88% 0.3s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 89% 0.3s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 90% 0.3s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 90% 0.2s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 91% 0.2s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 92% 0.2s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 93% 0.2s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 94% 0.1s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 95% 0.1s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 96% 0.1s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 97% 0.1s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 98% 0.0s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 99% 0.0s \u001b[0K\u001b[1GDownloading chrome r113.0.5672.63 - 303.9 MB [] 100% 0.0s \u001b[0K\n",
            "Chrome (113.0.5672.63) downloaded to /root/.cache/puppeteer/chrome/linux-113.0.5672.63\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mnotsup\u001b[0m Unsupported engine for puppeteer-core@20.2.1: wanted: {\"node\":\">=16.0.0\"} (current: {\"node\":\"14.16.0\",\"npm\":\"6.14.8\"})\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mnotsup\u001b[0m Not compatible with your version of node/npm: puppeteer-core@20.2.1\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mnotsup\u001b[0m Unsupported engine for @puppeteer/browsers@1.3.0: wanted: {\"node\":\">=16.0.0\"} (current: {\"node\":\"14.16.0\",\"npm\":\"6.14.8\"})\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35mnotsup\u001b[0m Not compatible with your version of node/npm: @puppeteer/browsers@1.3.0\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ puppeteer@20.2.1\n",
            "added 92 packages from 97 contributors and audited 199 packages in 20.588s\n",
            "\n",
            "10 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found \u001b[91m13\u001b[0m vulnerabilities (11 \u001b[93mmoderate\u001b[0m, 2 \u001b[91mhigh\u001b[0m)\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h\n",
            "\u001b[33m\u001b[39m\n",
            "\u001b[33m   ╭───────────────────────────────────────────────────────────────╮\u001b[39m\n",
            "   \u001b[33m│\u001b[39m                                                               \u001b[33m│\u001b[39m\n",
            "   \u001b[33m│\u001b[39m      New \u001b[31mmajor\u001b[39m version of npm available! \u001b[31m6.14.8\u001b[39m → \u001b[32m9.6.7\u001b[39m       \u001b[33m│\u001b[39m\n",
            "   \u001b[33m│\u001b[39m   \u001b[33mChangelog:\u001b[39m \u001b[36mhttps://github.com/npm/cli/releases/tag/v9.6.7\u001b[39m   \u001b[33m│\u001b[39m\n",
            "   \u001b[33m│\u001b[39m               Run \u001b[32mnpm install -g npm\u001b[39m to update!               \u001b[33m│\u001b[39m\n",
            "   \u001b[33m│\u001b[39m                                                               \u001b[33m│\u001b[39m\n",
            "\u001b[33m   ╰───────────────────────────────────────────────────────────────╯\u001b[39m\n",
            "\u001b[33m\u001b[39m\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ puppeteer-extra@3.3.6\n",
            "added 4 packages from 9 contributors and audited 276 packages in 2.564s\n",
            "\n",
            "10 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found \u001b[91m13\u001b[0m vulnerabilities (11 \u001b[93mmoderate\u001b[0m, 2 \u001b[91mhigh\u001b[0m)\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25h+ puppeteer-extra@3.3.6\n",
            "updated 1 package and audited 277 packages in 1.297s\n",
            "\n",
            "10 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found \u001b[91m13\u001b[0m vulnerabilities (11 \u001b[93mmoderate\u001b[0m, 2 \u001b[91mhigh\u001b[0m)\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ puppeteer-extra-plugin-stealth@2.11.2\n",
            "added 34 packages from 23 contributors and audited 311 packages in 3.797s\n",
            "\n",
            "12 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found \u001b[91m13\u001b[0m vulnerabilities (11 \u001b[93mmoderate\u001b[0m, 2 \u001b[91mhigh\u001b[0m)\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ puppeteer-extra-plugin-recaptcha@3.6.8\n",
            "added 1 package from 1 contributor and audited 332 packages in 1.918s\n",
            "\n",
            "12 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found \u001b[91m13\u001b[0m vulnerabilities (11 \u001b[93mmoderate\u001b[0m, 2 \u001b[91mhigh\u001b[0m)\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h--2023-05-19 05:41:42--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
            "Resolving dl.google.com (dl.google.com)... 172.253.62.93, 172.253.62.91, 172.253.62.136, ...\n",
            "Connecting to dl.google.com (dl.google.com)|172.253.62.93|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94682364 (90M) [application/x-debian-package]\n",
            "Saving to: ‘google-chrome-stable_current_amd64.deb’\n",
            "\n",
            "google-chrome-stabl 100%[===================>]  90.30M   116MB/s    in 0.8s    \n",
            "\n",
            "2023-05-19 05:41:43 (116 MB/s) - ‘google-chrome-stable_current_amd64.deb’ saved [94682364/94682364]\n",
            "\n",
            "Selecting previously unselected package google-chrome-stable.\n",
            "(Reading database ... 122531 files and directories currently installed.)\n",
            "Preparing to unpack google-chrome-stable_current_amd64.deb ...\n",
            "Unpacking google-chrome-stable (113.0.5672.126-1) ...\n",
            "\u001b[1mdpkg:\u001b[0m dependency problems prevent configuration of google-chrome-stable:\n",
            " google-chrome-stable depends on libu2f-udev; however:\n",
            "  Package libu2f-udev is not installed.\n",
            "\n",
            "\u001b[1mdpkg:\u001b[0m error processing package google-chrome-stable (--install):\n",
            " dependency problems - leaving unconfigured\n",
            "Processing triggers for mime-support (3.64ubuntu1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Errors were encountered while processing:\n",
            " google-chrome-stable\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Correcting dependencies... Done\n",
            "The following additional packages will be installed:\n",
            "  libu2f-udev libudev1 udev\n",
            "The following NEW packages will be installed:\n",
            "  libu2f-udev udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 2 newly installed, 0 to remove and 26 not upgraded.\n",
            "1 not fully installed or removed.\n",
            "Need to get 1,448 kB of archives.\n",
            "After this operation, 9,440 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libudev1 amd64 245.4-4ubuntu3.21 [75.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 udev amd64 245.4-4ubuntu3.21 [1,366 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/main amd64 libu2f-udev all 1.1.10-1 [6,108 B]\n",
            "Fetched 1,448 kB in 0s (9,290 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "(Reading database ... 122647 files and directories currently installed.)\n",
            "Preparing to unpack .../libudev1_245.4-4ubuntu3.21_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (245.4-4ubuntu3.21) over (245.4-4ubuntu3.19) ...\n",
            "Setting up libudev1:amd64 (245.4-4ubuntu3.21) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 122647 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_245.4-4ubuntu3.21_amd64.deb ...\n",
            "Unpacking udev (245.4-4ubuntu3.21) ...\n",
            "Selecting previously unselected package libu2f-udev.\n",
            "Preparing to unpack .../libu2f-udev_1.1.10-1_all.deb ...\n",
            "Unpacking libu2f-udev (1.1.10-1) ...\n",
            "Setting up udev (245.4-4ubuntu3.21) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libu2f-udev (1.1.10-1) ...\n",
            "Failed to send reload request: No such file or directory\n",
            "Setting up google-chrome-stable (113.0.5672.126-1) ...\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/google-chrome (google-chrome) in auto mode\n",
            "Processing triggers for systemd (245.4-4ubuntu3.21) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Google Chrome 113.0.5672.126 \n"
          ]
        }
      ],
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "!apt-get update\n",
        "# !apt install chromium-chromedriver\n",
        "# !cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "# import sys\n",
        "# sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "!npm i convert-excel-to-json\n",
        "!npm install puppeteer\n",
        "!npm install puppeteer-extra\n",
        "!npm install puppeteer-extra --save\n",
        "!npm install puppeteer-extra-plugin-stealth --save   # installs plugin\n",
        "!npm install puppeteer-extra-plugin-recaptcha\n",
        "\n",
        "## Install google chrome\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb   # Install the package using the dpkg command:\n",
        "!sudo dpkg -i google-chrome-stable_current_amd64.deb       # If you encounter any dependency errors during the installation, you can resolve them by running the following command:\n",
        "!sudo apt-get install -f      # Verify the installation by running the following command:\n",
        "!google-chrome --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maTpimjDOyg_"
      },
      "source": [
        "## Search by Subject"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ5DOPgJOUQf"
      },
      "source": [
        "Save links to json file because I coudn't  split date which is :link['E'], was converted to UTC and was showing err. link.E.getFullYear is not a function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nF6_PFwbiXeG",
        "outputId": "1398aa95-70b3-4318-b6d4-353d82900844"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ./filter_by_search.js\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./filter_by_search.js\n",
        "// check file that does not exist from links\n",
        "// may miss two classes same day\n",
        "\n",
        "var fs = require('fs');\n",
        "const excelToJson = require('convert-excel-to-json');\n",
        "const path = require('path');\n",
        "const download_root = './drive/MyDrive/zoom_downloads/'\n",
        "\n",
        "// function to save_json_data\n",
        "let save_json_data = function (data, file_path){\n",
        "  let to_save_data  = JSON.stringify(data);\n",
        "try {\n",
        "    fs.writeFileSync(file_path, to_save_data);\n",
        "} catch (error) {\n",
        "    console.log('Error saving data to file error_data...' + file_path + error);\n",
        "}\n",
        "}\n",
        "// function to load json data :: to test if data is actually saved\n",
        "let load_json_data = (file_path) => {\n",
        "  // loading error links\n",
        "  // var saved_data;\n",
        "  try {\n",
        "      var saved_data = fs.readFileSync(file_path, 'utf-8');\n",
        "      saved_data = JSON.parse(saved_data);\n",
        "      // console.log('Loaded Links...: \\n ' + saved_data);\n",
        "      return saved_data;\n",
        "  } catch (error) {\n",
        "      console.log('Error Loading json file ...: \\n ' + file_path + error); \n",
        "  }\n",
        "}\n",
        "\n",
        "// function to search\n",
        "let filter = function (sheet_links, search_terms){\n",
        "      let search_results = [];\n",
        "      let matchFound = false;\n",
        "      \n",
        "      for (let [current_link_index, link] of sheet_links.entries() ){\n",
        "      // console.log(current_link_index, link)\n",
        "      if (link['A'] == 'Class Name'){\n",
        "        continue;\n",
        "      }\n",
        "      // console.log(link['A']);\n",
        "      matchFound = false;\n",
        "      for (let j = 0; j < search_terms.length; j++) {\n",
        "        //console.log(`A: ${typeof(link['A'])} :: ${link['A']} ${link['A'].toString().includes(search_terms[j])}  ${search_terms[j]}\\n `)\n",
        "        //console.log(`B: ${typeof(link['B'])} :: ${link['B']} \\n `)\n",
        "        //console.log(`G: ${typeof(link['G'])} :: ${link['G']} \\n `)\n",
        "        // if (( (link['A'] != undefined) && (link['A'].toString().toLowerCase().includes(search_terms[j])) || (link['B'].toString().toLowerCase().includes(search_terms[j])) || (link['G'].toString().toLowerCase().includes(search_terms[j]) )))  {\n",
        "        if ( (link['B'].toString().toLowerCase().includes(search_terms[j])) )  {\n",
        "            matchFound = true;\n",
        "            // console.log('match found')\n",
        "            break;\n",
        "        }\n",
        "      }\n",
        "      if (matchFound==true) {\n",
        "        search_results.push(link);\n",
        "      }\n",
        "    }\n",
        "    return search_results;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "var all_sheets_links = excelToJson({  sourceFile: download_root + 'Fuse-Links.xlsx'  });\n",
        "// console.log(all_sheets_links)\n",
        "\n",
        "let subjects = ['physics', 'math', 'electronics', 'circuit','machine', 'economics', 'Electromagnetism', 'control system', 'intelligence', 'digital signal analysis', 'elective', 'principles of management', 'telecommunication']\n",
        "let teachers = ['PUSHKAR RAJ POKHREL', '']\n",
        "let results = [];\n",
        "\n",
        "for (let current_sheet in all_sheets_links){\n",
        "    // console.log(current_sheet)\n",
        "    let sheet_links = all_sheets_links[current_sheet]\n",
        "    console.log(sheet_links.length)\n",
        "    results = [...results, ...filter(sheet_links, subjects)]\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "var links_to_download = []\n",
        "\n",
        "save_json_data(results, download_root + 'links.json')\n",
        "\n",
        "\n",
        "all_links_saved = load_json_data(download_root + 'links.json')\n",
        "console.log('all_links_saved', all_links_saved.length)\n",
        "if (all_links_saved) {\n",
        "  console.log('---------------------------------------')\n",
        "  console.log('successfully saved all links')\n",
        "  console.log('---------------------------------------')\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pmyvxwISOTX",
        "outputId": "75c0a1a5-3bab-474a-a639-6060b01876e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m7288\u001b[39m\n",
            "all_links_saved \u001b[33m1669\u001b[39m\n",
            "---------------------------------------\n",
            "successfully saved all links\n",
            "---------------------------------------\n"
          ]
        }
      ],
      "source": [
        "!node filter_by_search.js"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCF9oZKHOQdc"
      },
      "source": [
        "(python code)<br>\n",
        "saving link_indexes yet to be download at file: ./drive/MyDrive/zoom_downloads/progress_logs_v3.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXTxy5UkzKvm",
        "outputId": "3df1beb8-6c35-4135-8c82-b56bf73e1009"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ./python_code.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./python_code.py\n",
        "import json\n",
        "import os\n",
        "download_root = './drive/MyDrive/zoom_downloads/'\n",
        "os.path.exists(download_root)\n",
        "\n",
        "\n",
        "toDownload = []\n",
        "toDownloadData = []\n",
        "# data = []\n",
        "with open(download_root + 'links.json','r') as file:\n",
        "  links = json.load(file)\n",
        "links = links[:]\n",
        "for index,link in enumerate(links[1:]):\n",
        "      if index == 1: continue  # pass first row (title row)\n",
        "      \n",
        "      link['B'] = link['B'].replace('/','|')\n",
        "      link['C'] = link['C'].replace('/','|')\n",
        "      link['G'] = link['G'].replace('/','|')\n",
        "      corresponding_folder_name = link['B'] + '[' + link['C'] + '][' + link['G'] + ']'\n",
        "\n",
        "      # file doesnot exists so need to downloads\n",
        "      if not os.path.exists(download_root + corresponding_folder_name):\n",
        "        # print(f'folder doesn\\'t exist {corresponding_folder_name}')\n",
        "        toDownload.append(index)\n",
        "        toDownloadData.append({'url': link['J'], 'password':link['I'], 'path':corresponding_folder_name})\n",
        "        continue\n",
        "      \n",
        "      \n",
        "      \n",
        "      year = link['E'].split('-')[0]\n",
        "      month = link['E'].split('-')[1]\n",
        "      day = link['E'].split('-')[2][:2]\n",
        "      \n",
        "      # print(year, month, day)\n",
        "      \n",
        "      corresponding_file_name_begin = 'GMT' + year + month + day + '-'\n",
        "      corresponding_file_name_end = '.mp4'\n",
        "\n",
        "      file_exists = False\n",
        "      \n",
        "      actual_files = os.listdir(download_root + corresponding_folder_name)\n",
        "      for actual_file_name in actual_files:\n",
        "        if (actual_file_name.startswith(corresponding_file_name_begin) and actual_file_name.endswith(corresponding_file_name_end)):\n",
        "      \n",
        "              file_exists = True\n",
        "              break\n",
        "          \n",
        "      if (not file_exists):\n",
        "          toDownload.append(index)\n",
        "          toDownloadData.append({'url': link['J'], 'password':link['I'], 'path':corresponding_folder_name})\n",
        "      # data.append({'folder':corresponding_folder_name, 'file_name_begin': corresponding_file_name_begin})\n",
        "print('index',index)\n",
        "print('todownload', len(toDownload))\n",
        "print(toDownload)\n",
        "\n",
        "with open(download_root + 'progress_logs_v3.json','w') as file:\n",
        "  json.dump({'borrowed':[], 'to_download':toDownload}, file)\n",
        "\n",
        "with open(download_root + 'to_download_data_v3.json','w') as file:\n",
        "  json.dump({'data':toDownloadData}, file)\n",
        "\n",
        "print('---------------------------------------')\n",
        "print('successfully saved to_download indexes at progress_logs_v3.json')\n",
        "print('---------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7ZYJ4I2aWed"
      },
      "outputs": [],
      "source": [
        "!python3 python_code.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILnwrAck9CgW"
      },
      "source": [
        "V3.1 : Use to_download_data_v3.json instead of progress_logs_v3.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqQCh1v09DGB",
        "outputId": "ac2c059c-e786-4997-c9f1-e28168478ef8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ./drive/MyDrive/zoom_downloads/download_code_v3_1.js\n"
          ]
        }
      ],
      "source": [
        "# actual code to download\n",
        "# stores code to file: 'app_v3.js'\n",
        "%%writefile ./drive/MyDrive/zoom_downloads/download_code_v3_1.js\n",
        "'use strict';\n",
        "\n",
        "// for colab code\n",
        "const linksPath = './drive/MyDrive/zoom_downloads/Fuse-Links.xlsx';\n",
        "const download_root = './drive/MyDrive/zoom_downloads/';  // create a folder called  'zoom_downloads' in MyDrive where downloads are to be stored\n",
        "\n",
        "// for running code locally\n",
        "// var download_root = '/home/gayatri/Documents/college/zoom_downloads/downloads/';\n",
        "// var linksPath = '/home/gayatri/Documents/college/zoom_downloads/Fuse-Links.xlsx';  // colab: './drive/MyDrive/zoom_downloads/Fuse-Links.xlsx'\n",
        "\n",
        "\n",
        "const excelToJson = require('convert-excel-to-json');\n",
        "// const puppeteer = require('puppeteer');\n",
        "const puppeteer = require('puppeteer-extra')\n",
        "// add stealth plugin and use defaults (all evasion techniques)\n",
        "const StealthPlugin = require('puppeteer-extra-plugin-stealth')\n",
        "puppeteer.use(StealthPlugin())\n",
        "\n",
        "var fs = require('fs'); // to create folder if not exist // reference: https://colab.research.google.com/drive/168X6Zo0Yk2fzEJ7WDfY9Q_0UOEmHSrZc?usp=sharing\n",
        "const { ConsoleMessage } = require('puppeteer')\n",
        "var progress_stored_previoiusly = 0;\n",
        "\n",
        "// add recaptcha plugin and provide it your 2captcha token (= their apiKey)\n",
        "// 2captcha is the builtin solution provider but others would work as well.\n",
        "// Please note: You need to add funds to your 2captcha account for this to work\n",
        "const RecaptchaPlugin = require('puppeteer-extra-plugin-recaptcha')\n",
        "puppeteer.use(\n",
        "  RecaptchaPlugin({\n",
        "    provider: {\n",
        "      id: '2captcha',\n",
        "      token: '' // REPLACE THIS WITH YOUR OWN 2CAPTCHA API KEY ⚡\n",
        "    },\n",
        "    visualFeedback: true // colorize reCAPTCHAs (violet = detected, green = solved)\n",
        "  })\n",
        ")\n",
        "\n",
        "\n",
        "const delay = ms => new Promise(resolve => {\n",
        "    console.log(\"sleeping for \" + ms/1000 + \" s\");\n",
        "    setTimeout(resolve, ms);\n",
        "});\n",
        "\n",
        "// syncronous delay\n",
        "function delay_sync(ms) {\n",
        "  console.log(\"syncronous delay  \" + ms/1000 + \" s\");\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "// const randomInteger = require('random-int');\n",
        "const randomInteger = (min, max) => {\n",
        "  return Math.floor(Math.random() * (max - min + 1)) + min;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "// create 'progress_logs_v3.json' if file doesnot exist\n",
        "if (!fs.existsSync(download_root + 'progress_logs_v3.json')){\n",
        "  console.log('\\ncreating file: progress_logs_v3.json...\\n');\n",
        "  fs.writeFileSync(download_root + 'progress_logs_v3.json', JSON.stringify(\n",
        "    {\n",
        "      \"comment\":\"progress for sheets are generated and updated based on sheet_name automatically ..\"\n",
        "    }\n",
        "  ));\n",
        " }\n",
        "\n",
        "// Create 'error_links_logs' if file doesnot exist\n",
        "if (!fs.existsSync(download_root + 'error_logs.json')){\n",
        "  console.log('\\ncreating file: error_logs.json...\\n');\n",
        "  fs.writeFileSync(download_root + 'error_logs.json', JSON.stringify(\n",
        "      {\n",
        "          link_logs:[],\n",
        "          other_logs:[],\n",
        "      }\n",
        "      ));\n",
        "}\n",
        "\n",
        "let load_json_data = (file_path) => {\n",
        "  // loading error links\n",
        "  // var saved_data;\n",
        "  try {\n",
        "      var saved_data = fs.readFileSync(file_path, 'utf-8');\n",
        "      saved_data = JSON.parse(saved_data);\n",
        "      // console.log('Loaded Links...: \\n ' + saved_data);\n",
        "      return saved_data;\n",
        "  } catch (error) {\n",
        "      console.log('Error Loading json file ...: \\n ' + file_path + error); \n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "let save_json_data = function (data, file_path){\n",
        "  let to_save_data  = JSON.stringify(data);\n",
        "try {\n",
        "    fs.writeFileSync(file_path, to_save_data);\n",
        "} catch (error) {\n",
        "    console.log('Error saving data to file error_data...' + file_path + error);\n",
        "}\n",
        "}\n",
        "\n",
        "// load progress logs\n",
        "// data = {'borrowed':[0,1]}\n",
        "var data = load_json_data(download_root + 'progress_logs_v3.json');\n",
        "var borrowed = [...data.borrowed]       // links download in progress\n",
        "console.log('\\nborrowed: ',borrowed)\n",
        "\n",
        "// update current progress of sheet after asynchronously waiting for download_time seconds. \n",
        "let async_wait_and_update_current_download_progress =  async (how_long_after_to_assume_downloaded, borrowed) => {\n",
        "  setTimeout(function(){\n",
        "    save_json_data({'borrowed':[...borrowed]}, download_root + 'progress_logs_v3.json');\n",
        "    console.log('updated borrowed: ', borrowed)\n",
        "      // let current_date_ms = Date.parse(new Date());\n",
        "      // console.log(`updated download index to: ${current_link_index} for sheet: ${current_sheet}`);\n",
        "\n",
        "  }, how_long_after_to_assume_downloaded);//wait 2 seconds\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "let append_error_logs = (new_log, where) => {\n",
        "  // where = 'link_logs' or 'other_logs'\n",
        "\n",
        "  let previous_logs = load_json_data(download_root + 'error_logs.json');  // load previous logs\n",
        "  // console.log('previous_logs' + previous_logs);\n",
        "  // console.log('prev' + previous_logs);\n",
        "  previous_logs[where].push(new_log);                   // update logs\n",
        "  // console.log(previous_logs[where]);\n",
        "  \n",
        "  let initial_date = Date.now()\n",
        "  \n",
        "  while (Date.now() <= initial_date + 10000){}\n",
        "  save_json_data(previous_logs, download_root + 'error_logs.json');    // store error logs\n",
        "  previous_logs = JSON.stringify(previous_logs);\n",
        "  console.log('\\nappended error log ...\\n');\n",
        "  console.log('\\n waiting 15 minutes sync for remaining downloads to finis \\n')\n",
        "  // update links.indexOf\n",
        "  delay_sync(900000); // waiting 15 minutes for remaining downloads to finish\n",
        "}\n",
        "\n",
        "\n",
        "// js progress generator function\n",
        "function progress_bar(current_progress, total, label){\n",
        "  let progress = Math.round((current_progress/total)*100);\n",
        "  console.log(`${label}_progress  : ${progress}% :: `, current_progress, '/', total);\n",
        "  console.log(total);\n",
        "  let bar = [];\n",
        "  for (let i = 0; i < 100; i++){\n",
        "    if (i < progress){\n",
        "      bar.push('█');\n",
        "    } else {\n",
        "      bar.push('░');\n",
        "    }\n",
        "  }\n",
        "  // return bar.join(''); // array to string\n",
        "  console.log(bar.join(''));\n",
        "}\n",
        "// for (let i=0;i<10000;i++ ){progress_bar(i,10000, 'count')} # test progress_bar\n",
        "\n",
        "\n",
        "\n",
        "const download_links = async (links) => {\n",
        "    // initialize browser\n",
        "    // refrence: https://colab.research.google.com/drive/168X6Zo0Yk2fzEJ7WDfY9Q_0UOEmHSrZc?usp=sharing\n",
        "    // google cloud console\n",
        "    // const browser = await puppeteer.launch({executablePath:\"/opt/google/chrome/google-chrome\", args:['--no-sandbox','--start-maximized'], ignoreHTTPSErrors: true, headless: true});\n",
        "    // colab specific\n",
        "    // const browser = await puppeteer.launch({executablePath:\"/usr/bin/chromium-browser\", args:['--no-sandbox','--start-maximized'], ignoreHTTPSErrors: true, headless: true});\n",
        "    // const browser = await puppeteer.launch({headless:false, ignoreHTTPSErrors: true}); // colab: const browser = await puppeteer.launch({executablePath:\"/usr/bin/chromium-browser\", args:['--no-sandbox','--start-maximized'], ignoreHTTPSErrors: true, headless: true});  // colab code:  \n",
        "    const browser = await puppeteer.launch({executablePath: \"/opt/google/chrome/chrome\", args:['--no-sandbox','--start-maximized'], ignoreHTTPSErrors: true, headless: true});\n",
        "  \n",
        "    const page = await browser.newPage();\n",
        "    await page.setExtraHTTPHeaders({\n",
        "        'Accept-Language': 'en-US,en;q=0.9',\n",
        "    });\n",
        "    await page.setUserAgent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36');\n",
        "    \n",
        "    // accept cookies\n",
        "    // console.log('opening https://zoom.us/ and accepting cookies ...');\n",
        "    // await page.goto('https://zoom.us/', {timeout: 60000});//, {waitUntil: 'networkidle0'});\n",
        "    // await page.screenshot({path: screenShotPath + 'before-accept-cookies' +'.png'});\n",
        "    // await page.waitForSelector('#onetrust-accept-btn-handler');\n",
        "    // document.querySelector('#onetrust-accept-btn-handler').click()\n",
        "    // await page.screenshot({path: screenShotPath + 'after-accept-cookies' +'.png'});\n",
        "    // console.log('accepted cookies ...')\n",
        "\n",
        "    // await delay(2000);  // wait 2 seconds\n",
        "\n",
        "    // file_name\n",
        "    // f'[count_index] + [{class_name[:170]}] + [{course_name}] + [{Grade}] + [{Duration}] + [{StartDateTime}] + [{EndDateTime}] + [{InstructorName}]\n",
        "\n",
        "      const screenShotPath = download_root + \"screenshots/\";\n",
        "      // for (let [current_link_index, link] of links.entries() ){\n",
        "      //for(let current_link_index of to_download){\n",
        "      for (let link of links){\n",
        "        console.log('link', link['url'], link['path']);\n",
        "        // let link = links[parseInt(current_link_index)]\n",
        "        try {\n",
        "            let url = link['url']\n",
        "            let path = link['path']\n",
        "            let password = link['password']\n",
        "            let current_link_index = links.indexOf(link)\n",
        "            if (borrowed.indexOf(current_link_index) != -1){\n",
        "              // skip if borrowed\n",
        "              console.log(`\\n link: ${current_link_index} in borrowed skipping...\\n`);\n",
        "              continue;\n",
        "            } else {\n",
        "              // add link to borrowed list\n",
        "              borrowed.push(current_link_index);\n",
        "              save_json_data({'borrowed':[...borrowed]}, download_root + 'progress_logs_v3.json');\n",
        "            }\n",
        "            progress_bar(current_link_index, links.length, 'download');  // displays progress of download\n",
        "\n",
        "          if (url.slice(0,4) !='http') {\n",
        "            append_error_logs({courseName: path, password: password, url: url, error_value: 'url not link'},'other_logs');\n",
        "            continue; // skip non download url\n",
        "        }\n",
        "        const downloadPath = download_root + path;\n",
        "        console.log('point1');\n",
        "        // create folder if not exists\n",
        "        if (!fs.existsSync(downloadPath)){\n",
        "            fs.mkdirSync(downloadPath);\n",
        "            console.log(`creating folder: ${downloadPath}`);\n",
        "            \n",
        "            if (!fs.existsSync(screenShotPath)){    // create screenshot path if don't exist\n",
        "                fs.mkdirSync(screenShotPath);\n",
        "            }\n",
        "        }\n",
        "        console.log('point2');\n",
        "        // set download location \n",
        "        const client = await page.target().createCDPSession();\n",
        "        await client.send('Page.setDownloadBehavior', {\n",
        "            behavior: 'allow',\n",
        "            downloadPath: downloadPath,\n",
        "            eventsEnabled: true,\n",
        "        })  \n",
        "        console.log(`set download path: ${downloadPath}`)\n",
        "        console.log('point3');\n",
        "          \n",
        "        await page.goto(url, {timeout: 30000, waitUntil: 'networkidle2'});\n",
        "        await page.screenshot({path: download_root + 'screen_before_click_download.png'});\n",
        "        console.log('point4');\n",
        "        // await page.waitForSelector('#passcode');\n",
        "        try{\n",
        "              await page.type('#password', password);\n",
        "              await delay(randomInteger(1000, 5000)); // random delay betn 1 and 5 seconds\n",
        "              \n",
        "              console.log('point5');\n",
        "              // submit password\n",
        "              await page.click('.submit');\n",
        "            } catch(err) {\n",
        "              await page.type('#passcode', password);\n",
        "              await delay(randomInteger(1000, 5000)); // random delay betn 1 and 5 seconds\n",
        "              \n",
        "              // solving capatcha\n",
        "              // That's it, a single line of code to solve reCAPTCHAs 🎉\n",
        "              // try{\n",
        "              //   await page.solveRecaptchas()\n",
        "              //   await Promise.all([\n",
        "              //     page.waitForNavigation(),\n",
        "              //     page.click(`#recaptcha-demo-submit`)\n",
        "              //   ])\n",
        "              // } catch(err){console.log(err)}\n",
        "              \n",
        "              // submit password\n",
        "              await page.click('#passcode_btn');\n",
        "              console.log(`downloading... link_index:${current_link_index}` + String(url));\n",
        "              \n",
        "              // update borrowed list\n",
        "              await async_wait_and_update_current_download_progress(35000, borrowed )\n",
        "              \n",
        "               // random delay betn 35 and 80 seconds after each download click  \n",
        "              await delay(randomInteger(35000, 80000));\n",
        "            }\n",
        "          \n",
        "          // screenshot before each delay :: 10 screenshots\n",
        "          await page.screenshot({path: screenShotPath + current_link_index +'.png'});\n",
        "          \n",
        "          // display any error by zoom <zoom sometimes give '401 unauthorized' error >\n",
        "          // zoom displaying error in two ways sometimes first way, second_way other times\n",
        "          let error_element = await page.$('.zm-alert__content');\n",
        "          let second_error_element = await page.$('#error_msg');\n",
        "          if (error_element == null && second_error_element != null)error_element = second_error_element\n",
        "          \n",
        "          if (error_element != null){ \n",
        "            let error_value = await page.evaluate(el => el.textContent, error_element)\n",
        "            if (!(String(error_value) == '')){\n",
        "              // storing error log\n",
        "              append_error_logs({courseName: path, password: password, url: url, error_value: error_value}, 'link_logs');\n",
        "            \n",
        "              // displaying error message in console \n",
        "              console.log('Error\\n' + error_value);\n",
        "              console.log('On Link: ' + link);\n",
        "              // delay_sync(180000); // 3 minutes delay\n",
        "              break\n",
        "              // process.exit(\"Exit: this is the error of zoom (maybe wait few minutes and re-run the script)\");\n",
        "              //continue;\n",
        "            }\n",
        "          }\n",
        "\n",
        "        \n",
        "      } catch (error) {\n",
        "        // storing error log\n",
        "        append_error_logs({link:link, error_value: JSON.stringify(error)},'link_logs');\n",
        "\n",
        "        // displaying error message in console \n",
        "        console.log('\\n' + 'Error' + error + '\\n');\n",
        "        console.log('On Link: ' + link);\n",
        "        \n",
        "        console.log('Waiting 5 minutes to let pending downloads to finish');\n",
        "        delay_sync(300000); // 5 minutes delay\n",
        "        break\n",
        "      }  \n",
        "      }\n",
        "\n",
        "      console.log('\\n\\n --------------- ************************* --------------- ');\n",
        "      console.log(' --- completed clicking download btn -> download in progress --- \\n ------------ waiting 10 minutes ------------ ');\n",
        "      console.log(' --------------- ************************* --------------- \\n\\n ');\n",
        "      delay_sync(180000);\n",
        "      process.exit(\"Exit: this is the error of zoom (maybe wait few minutes and re-run the script)\");\n",
        "    // waiting 15 minutes before closing browser after clicking download to all links of specific sheet\n",
        "    console.log(`\\n Closing sheet: ${current_sheet} browser after waiting for 10 minutes after \\\"last link download click\\\" for download to complete. \\n`);\n",
        "    await browser.close();\n",
        "}\n",
        "\n",
        "let urls_path = download_root + \"to_download_data_v3.json\";\n",
        "var links_to_download = load_json_data(urls_path);\n",
        "links_to_download = links_to_download['data'];\n",
        "\n",
        "try{\n",
        "    download_links(links_to_download);\n",
        "} catch (error) {\n",
        "  // store error reading the link: link_path, error_message\n",
        "  append_error_logs({linksPath:urls_path, error_msg: 'error reading links file', error_value: String(error)}, 'other_logs');\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "/*\n",
        "- auto-find & download links not to download by folder_name & file_name\n",
        "- 'to_download_data_v3.json' stores url, path:<folder_name>, password\n",
        "- 'progress_logs_v3.json' stores index of links not downloaded :: not used anymore\n",
        "- removed solveRecaptchas() :: not working\n",
        "- zoom password submit showing two varients\n",
        "- zoom password input id: #passcode           previous: #password\n",
        "- zoom password submit  : #passcode_btn       previous: #submit\n",
        "- error message:          .zm-alert__content  previous: #error_msg\n",
        "*/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNpbnXnfbWgL"
      },
      "outputs": [],
      "source": [
        "!node ./drive/MyDrive/zoom_downloads/download_code_v3_1.js"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "548l0tHE8P83"
      },
      "source": [
        "## ***Post Processing***\n",
        "* ***Delete Files with size less than 2MB***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyu9aSH68PaN",
        "outputId": "56700b77-c244-4855-c33a-3c6dc097f0b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting delete_below_threshold.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile delete_below_threshold.py\n",
        "import os\n",
        "\n",
        "def delete_small_files(directory, threshold_size):\n",
        "    delete_count = 0\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith(\".mp4\"):\n",
        "                file_path = os.path.join(root, file)\n",
        "                file_size = os.path.getsize(file_path)\n",
        "                if file_size < threshold_size:\n",
        "                    os.remove(file_path)\n",
        "                    # print(f\"Deleted file: {file_path}\")\n",
        "                    delete_count += 1\n",
        "    print(f'\\n no. of files deleted that are less than 2 MB : {delete_count} ')\n",
        "\n",
        "# Usage\n",
        "directory_path = './drive/MyDrive/zoom_downloads/'\n",
        "threshold_size = 2 * 1024 * 1024  # 2MB in bytes\n",
        "\n",
        "delete_small_files(directory_path, threshold_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqSha1Lq-_kg",
        "outputId": "15636979-1a01-4230-81ef-be0fae5e0526"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " no. of files deleted that are less than 2 MB : 0 \n"
          ]
        }
      ],
      "source": [
        "!python3 post_process.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC0vxBfYamNY"
      },
      "source": [
        "# ***Infinite loop***\n",
        "\n",
        "* Shell Script code\n",
        "- to retry download when script gets <401:Forbidden> error from zoom\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4eZ7AvbVyvc",
        "outputId": "e7d2a3bf-0c8e-42a0-d7ca-261e16054bf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ./loop_script.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./loop_script.sh\n",
        "#!/bin/bash\n",
        "\n",
        "while true; do\n",
        "    python3 delete_below_threshold.py\n",
        "    node filter_by_search.js\n",
        "    python3 python_code.py\n",
        "    node drive/MyDrive/zoom_downloads/download_code_v3_1.js\n",
        "    sleep 3m\n",
        "done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZnqBbUIbJka",
        "outputId": "142a4496-2d04-4f31-f20e-b9820de0b12d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " no. of files deleted that are less than 2 MB : 0 \n",
            "\u001b[33m7288\u001b[39m\n",
            "all_links_saved \u001b[33m1705\u001b[39m\n",
            "---------------------------------------\n",
            "successfully saved all links\n",
            "---------------------------------------\n",
            "index 1703\n",
            "todownload 1000\n",
            "[19, 20, 184, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703]\n",
            "---------------------------------------\n",
            "successfully saved to_download indexes at progress_logs_v3.json\n",
            "---------------------------------------\n",
            "\n",
            "borrowed:  []\n",
            "\u001b[1m\u001b[43m\u001b[30m\n",
            "  Puppeteer old Headless deprecation warning:\u001b[0m\u001b[33m\n",
            "    In the near feature `headless: true` will default to the new Headless mode\n",
            "    for Chrome instead of the old Headless implementation. For more\n",
            "    information, please see https://developer.chrome.com/articles/new-headless/.\n",
            "    Consider opting in early by passing `headless: \"new\"` to `puppeteer.launch()`\n",
            "    If you encounter any bugs, please report them to https://github.com/puppeteer/puppeteer/issues/new/choose.\u001b[0m\n",
            "\n",
            "link https://zoom.us/rec/download/PRQL990wJN57HsnS5z7UMJ6aR1UdPxhmJME5e8I34vcCiGOVGKakGKTALrE6OkoKtf0dbJPfGj95TOeW.NwAWgq1cnJo0M6Gi Basic Electronics Engineering - DS - Section A[Year 1][Dakshina Shrestha]\n",
            "download_progress  : 0% ::  \u001b[33m0\u001b[39m / \u001b[33m1000\u001b[39m\n",
            "\u001b[33m1000\u001b[39m\n",
            "░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░\n",
            "point1\n",
            "point2\n",
            "set download path: ./drive/MyDrive/zoom_downloads/Basic Electronics Engineering - DS - Section A[Year 1][Dakshina Shrestha]\n",
            "point3\n",
            "point4\n",
            "sleeping for 3.454 s\n",
            "downloading... link_index:0https://zoom.us/rec/download/PRQL990wJN57HsnS5z7UMJ6aR1UdPxhmJME5e8I34vcCiGOVGKakGKTALrE6OkoKtf0dbJPfGj95TOeW.NwAWgq1cnJo0M6Gi\n",
            "sleeping for 77.185 s\n"
          ]
        }
      ],
      "source": [
        "!chmod +x loop_script.sh\n",
        "!/bin/bash loop_script.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrprtNqpUYn6"
      },
      "source": [
        "## ***Test if fuse_links path is correct***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3Y6aV26hlqW",
        "outputId": "3d7c448f-b43f-49c2-e9c2-effd8c0406e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./drive/MyDrive/zoom_downloads/app.js\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%writefile test_links.js\n",
        "'use strict';\n",
        "const excelToJson = require('convert-excel-to-json');\n",
        "const puppeteer = require('puppeteer');\n",
        "var fs = require('fs'); // to create folder if not exist // reference: https://colab.research.google.com/drive/168X6Zo0Yk2fzEJ7WDfY9Q_0UOEmHSrZc?usp=sharing\n",
        "const delay = ms => new Promise(resolve => setTimeout(resolve, ms));\n",
        "\n",
        "// for colab code\n",
        "const linksPath = './drive/MyDrive/zoom_downloads/Fuse-Links.xlsx';\n",
        "const download_root = './drive/MyDrive/zoom_downloads/';  // create a folder called  'zoom_downloads' in MyDrive where downloads are to be stored\n",
        "\n",
        "\n",
        "let load_json_data = (file_path) => {\n",
        "  // loading error links\n",
        "  // var saved_data;\n",
        "  try {\n",
        "      var saved_data = fs.readFileSync(file_path, 'utf-8');\n",
        "      saved_data = JSON.parse(saved_data);\n",
        "      // console.log('Loaded Links...: \\n ' + saved_data);\n",
        "      return saved_data;\n",
        "  } catch (error) {\n",
        "      console.log('Error Loading json file ...: \\n ' + file_path + error); \n",
        "  }\n",
        "}\n",
        "let download_progress = load_json_data(download_root + 'progress_logs.json');\n",
        "var all_sheets_links = excelToJson({  sourceFile: linksPath  });\n",
        "for (let current_sheet in all_sheets_links ){\n",
        "  console.log(`{current_sheet} :: {typeof(current_sheet)}`);\n",
        "  console.log(download_progress[current_sheet]);\n",
        "  }\n",
        "//console.log(all_sheets_links);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vyNv9cKh5m3"
      },
      "outputs": [],
      "source": [
        "# run test_links: should display links in output \n",
        "!node test_links.js\n",
        "#!ls drive/MyDrive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1zXtgygpP6F"
      },
      "source": [
        "## ***Method1: Copy files to another google drive***\n",
        "  * iterate through all the folders in source dir\n",
        "  * if file doesnt exist in destination: move file from source to destination \n",
        "  * if file exist in destination: move file if size in destination is less than size in source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjfPS00XpPI1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "# Mount your first Google Drive\n",
        "drive.mount('/content/drive1')\n",
        "\n",
        "# Mount second Google Drive\n",
        "!sudo echo -ne '\\n' | sudo add-apt-repository ppa:alessandro-strada/ppa >/dev/null 2>&1 # note: >/dev/null 2>&1 is used to supress printing\n",
        "!sudo apt update >/dev/null 2>&1\n",
        "!sudo apt install google-drive-ocamlfuse >/dev/null 2>&1\n",
        "!google-drive-ocamlfuse\n",
        "!sudo apt-get install w3m >/dev/null 2>&1 # to act as web browser \n",
        "!xdg-settings set default-web-browser w3m.desktop >/dev/null 2>&1 # to set default browser \n",
        "%cd /content\n",
        "!mkdir gdrive\n",
        "%cd gdrive\n",
        "!mkdir \"drive2\"\n",
        "!google-drive-ocamlfuse \"/content/gdrive/drive2\"\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "def merge(source, destination):\n",
        "    for path in os.listdir(source):\n",
        "        source_path = os.path.join(source, path)\n",
        "        destination_path = os.path.join(destination,path)\n",
        "        \n",
        "        if not os.path.isdir(source_path):\n",
        "            # path is file: copy from source to destination\n",
        "            \n",
        "            if os.path.exists(destination_path):\n",
        "                print(source_path, destination_path)\n",
        "                # file exist in destination\n",
        "                if os.path.getsize(source_path) > os.path.getsize(destination_path):\n",
        "                    # source file is larger than destination file\n",
        "                    os.remove(destination_path)\n",
        "                else:\n",
        "                    continue    # destination file path has larger size than source file :: skip\n",
        "            shutil.copy(source_path, destination_path)\n",
        "        else:\n",
        "            # path is folder: \n",
        "                if not os.path.exists(destination_path):\n",
        "                    # destination folder does not exist\n",
        "                    os.mkdir(destination_path)\n",
        "                elif not os.path.isdir(destination_path):\n",
        "                    # destination exists but is file not folder\n",
        "                    destination_path = os.path.join(destination, path+'_folder')\n",
        "                    os.mkdir(destination_path)\n",
        "                merge(source_path, destination_path)\n",
        "\n",
        "source_dir = '/content/gdrive/drive2/zoom_downloads/'\n",
        "destination_dir = '/content/drive1/MyDrive/zoom_downloads/'\n",
        "merge(source_dir, destination_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cTJ3CLYEW-k"
      },
      "source": [
        "## ***Method2: Copy files to another google drive***\n",
        "  * iterate through all the folders in source dir\n",
        "  * if file doesnt exist in destination: move file from source to destination \n",
        "  * if file exist in destination: move file if size in destination is less than size in source"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRd1Lhc6EX2q"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount your first Google Drive\n",
        "drive.mount('/content/drive1')\n",
        "\n",
        "# Mount second Google Drive\n",
        "!sudo echo -ne '\\n' | sudo add-apt-repository ppa:alessandro-strada/ppa >/dev/null 2>&1 # note: >/dev/null 2>&1 is used to supress printing\n",
        "!sudo apt update >/dev/null 2>&1\n",
        "!sudo apt install google-drive-ocamlfuse >/dev/null 2>&1\n",
        "!google-drive-ocamlfuse\n",
        "!sudo apt-get install w3m >/dev/null 2>&1 # to act as web browser \n",
        "!xdg-settings set default-web-browser w3m.desktop >/dev/null 2>&1 # to set default browser \n",
        "%cd /content\n",
        "!mkdir gdrive\n",
        "%cd gdrive\n",
        "!mkdir \"drive2\"\n",
        "!google-drive-ocamlfuse \"/content/gdrive/drive2\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "root_dir = \"/content/drive/MyDrive/mero_school/\"\n",
        "source_dirs = ['basic electrical engineering0', 'electromagnetics0', 'electronic devices and circuits (iii_i)0']\n",
        "destination_dirs = ['basic electrical engineering', 'electromagnetics', 'electronic devices and circuits (iii_i)']\n",
        "\n",
        "source_dir = '/content/gdrive/drive2/zoom_downloads/'\n",
        "destination_dir = '/content/drive1/MyDrive/zoom_downloads/'\n",
        "\n",
        "def move_if_not_exist_and_keep_larger_file(source, destination):\n",
        "    source_dir = os.path.join(root_dir, source)\n",
        "    dest_dir = os.path.join(root_dir, destination)\n",
        "\n",
        "    for file_name in os.listdir(source_dir):\n",
        "        src_file_path = os.path.join(source_dir, file_name)\n",
        "        dest_file_path = os.path.join(dest_dir, file_name)\n",
        "\n",
        "        if not os.path.exists(dest_file_path):\n",
        "            shutil.move(src_file_path, dest_file_path)\n",
        "            print(f\"Moved {file_name} to {dest_dir}\")\n",
        "        else:\n",
        "            src_file_size = os.path.getsize(src_file_path)\n",
        "            dest_file_size = os.path.getsize(dest_file_path)\n",
        "            if src_file_size > dest_file_size:\n",
        "                os.remove(dest_file_path)\n",
        "                shutil.move(src_file_path, dest_file_path)\n",
        "                print(f\"Replaced {file_name} in {dest_dir} with larger file\")\n",
        "            else:\n",
        "                os.remove(src_file_path)\n",
        "                print(f\"Ignored {file_name} as it already exists in {dest_dir} and is larger\")\n",
        "\n",
        "    print(\"Done!\")\n",
        "\n",
        "for source, destination in zip(source_dirs, destination_dirs):\n",
        "    print(f\"\\'{str(source)}\\', \\'{str(destination)}\\'\")\n",
        "    move_if_not_exist_and_keep_larger_file(source, destination)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TlHdDp2An5N"
      },
      "source": [
        "## ***Method3: Copy files to another google drive***\n",
        "* Copies From drive2 to drive1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAxh2qQqAs6N"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount your first Google Drive\n",
        "drive.mount('/content/drive1')\n",
        "\n",
        "\n",
        "\n",
        "# Mount second Google Drive\n",
        "!sudo echo -ne '\\n' | sudo add-apt-repository ppa:alessandro-strada/ppa >/dev/null 2>&1 # note: >/dev/null 2>&1 is used to supress printing\n",
        "!sudo apt update >/dev/null 2>&1\n",
        "!sudo apt install google-drive-ocamlfuse >/dev/null 2>&1\n",
        "!google-drive-ocamlfuse\n",
        "!sudo apt-get install w3m >/dev/null 2>&1 # to act as web browser \n",
        "!xdg-settings set default-web-browser w3m.desktop >/dev/null 2>&1 # to set default browser \n",
        "%cd /content\n",
        "!mkdir gdrive\n",
        "%cd gdrive\n",
        "!mkdir \"drive2\"\n",
        "!google-drive-ocamlfuse \"/content/gdrive/drive2\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# Code to copy\n",
        "# --------------------\n",
        "\n",
        "# !!ls '/content/drive1/MyDrive/Colab Notebooks'\n",
        "# !cp -r '/content/drive1/MyDrive/Colab Notebooks/' '/content/gdrive/drive2/colab_notebooks_another/\n",
        "# rsync -avzP /media/oldhd/ /media/newhd/\n",
        "\n",
        "# !ls /content/drive1/MyDrive/zoom_downloads/\n",
        "# !ls /content/gdrive/drive2/zoom_downloads\n",
        "# !ls /content/gdrive/drive2\n",
        "# !ls /content/drive1/MyDrive/\n",
        "\n",
        "\n",
        "!rsync -avzP -r --progress /content/gdrive/drive2/zoom_downloads/ /content/drive1/MyDrive/zoom_downloads/\n",
        "\n",
        "# Use the -z flag to compress data during transfer. This can significantly reduce the amount of data that needs to be transferred, especially if you are transferring large files with a lot of repeated data.\n",
        "# Use the --delete flag to delete files in the destination directory that have been deleted in the source directory. This can save time by avoiding the need to transfer files that are no longer needed.\n",
        "# Use the -W flag to copy whole files, rather than just the differences between files. This can be faster if you are transferring a large number of small files, since it avoids the overhead of calculating and transferring the differences.\n",
        "# Use the -h flag to display output in a human-readable format. This can make it easier to see which files are being transferred, and can help you identify any problems or bottlenecks in the transfer process.\n",
        "# Use the -a flag to preserve file permissions, ownership, and timestamps. This can save time by avoiding the need to recalculate these values for each file.\n",
        "# Use a fast network connection. rsync is designed to transfer data efficiently over a network, so a faster connection will generally result in faster transfers.\n",
        "# Use a tool like rsync+ssh to encrypt data during transfer. This can add overhead to the transfer process, but can be worth it for the added security."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}